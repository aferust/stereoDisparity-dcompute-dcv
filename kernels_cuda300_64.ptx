//
// Generated by LLVM NVPTX Back-End
//

.version 3.2
.target sm_30
.address_size 64

	// .globl	_D7dispmap7__usad4FkkkZk
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11GlobalIndex__T1xZQdFNaNbNdNiZm
()
;
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11GlobalIndex__T1yZQdFNaNbNdNiZm
()
;
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11SharedIndex__T1xZQdFNaNbNdNiZm
()
;
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11SharedIndex__T1yZQdFNaNbNdNiZm
()
;
.weak .func _D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe
(
	.param .b64 _D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe_param_0
)
;
.extern .func  (.param .b32 func_retval0) vprintf
(
	.param .b64 vprintf_param_0,
	.param .b64 vprintf_param_1
)
;
.weak .func  (.param .b32 func_retval0) _D9bilateral__T3absTfZQhFNaNbNiNefZf
(
	.param .b32 _D9bilateral__T3absTfZQhFNaNbNiNefZf_param_0
)
;
.const .align 1 .b8 str[4] = {37, 100, 10, 0};
.visible .const .align 4 .b8 gauss0[256];
.global .align 1 .b8 _$_str[3] = {54, 52, 0};
.global .align 1 .b8 _$_str1[2] = {54, 0};
.global .align 1 .b8 _$_str2[2] = {52, 0};
.global .align 1 .b8 _$_str3[6] = {102, 108, 111, 97, 116, 0};
.visible .shared .align 4 .b8 diff0[4608];
.global .align 1 .b8 _$_str4[5] = {49, 49, 53, 50, 0};
.global .align 1 .b8 _$_str5[4] = {49, 49, 53, 0};
.global .align 1 .b8 _$_str6[3] = {49, 49, 0};
.global .align 1 .b8 _$_str7[2] = {49, 0};
.global .align 1 .b8 _$_str8[2] = {53, 0};
.global .align 1 .b8 _$_str9[2] = {50, 0};
.global .align 1 .b8 _$_str10[4] = {105, 51, 50, 0};
.extern .global .align 4 .b8 _D8dcompute3std4cuda7texture6float46__initZ[16];

.visible .func  (.param .b32 func_retval0) _D7dispmap7__usad4FkkkZk(
	.param .b32 _D7dispmap7__usad4FkkkZk_param_0,
	.param .b32 _D7dispmap7__usad4FkkkZk_param_1,
	.param .b32 _D7dispmap7__usad4FkkkZk_param_2
)
{
	.local .align 4 .b8 	__local_depot0[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<9>;

	mov.u64 	%SPL, __local_depot0;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r3, [_D7dispmap7__usad4FkkkZk_param_2];
	ld.param.u32 	%r2, [_D7dispmap7__usad4FkkkZk_param_1];
	ld.param.u32 	%r1, [_D7dispmap7__usad4FkkkZk_param_0];
	st.u32 	[%SP+0], %r1;
	st.u32 	[%SP+4], %r2;
	st.u32 	[%SP+8], %r3;
	ld.u32 	%r5, [%SP+0];
	ld.u32 	%r6, [%SP+4];
	ld.u32 	%r7, [%SP+8];
	// begin inline asm
	vabsdiff4.u32.u32.u32.add %r4, %r5, %r6, %r7;
	// end inline asm
	st.u32 	[%SP+12], %r4;
	ld.u32 	%r8, [%SP+12];
	st.param.b32 	[func_retval0+0], %r8;
	ret;

}
	// .globl	_D7dispmap5tex2DFmiiZk
.visible .func  (.param .b32 func_retval0) _D7dispmap5tex2DFmiiZk(
	.param .b64 _D7dispmap5tex2DFmiiZk_param_0,
	.param .b32 _D7dispmap5tex2DFmiiZk_param_1,
	.param .b32 _D7dispmap5tex2DFmiiZk_param_2
)
{
	.local .align 8 .b8 	__local_depot1[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<6>;

	mov.u64 	%SPL, __local_depot1;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [_D7dispmap5tex2DFmiiZk_param_2];
	ld.param.u32 	%r1, [_D7dispmap5tex2DFmiiZk_param_1];
	ld.param.u64 	%rd1, [_D7dispmap5tex2DFmiiZk_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u32 	[%SP+8], %r1;
	st.u32 	[%SP+12], %r2;
	ld.u64 	%rd2, [%SP+0];
	ld.u32 	%r3, [%SP+8];
	ld.u32 	%r4, [%SP+12];
	tex.2d.v4.u32.s32 	{%r5, %r6, %r7, %r8}, [%rd2, {%r3, %r4}];
	mov.u64 	%rd3, 0;
	st.u64 	[%SP+24], %rd3;
	st.u64 	[%SP+16], %rd3;
	st.u32 	[%SP+16], %r5;
	add.u64 	%rd4, %SP, 16;
	or.b64  	%rd5, %rd4, 4;
	st.u32 	[%rd5], %r6;
	st.u32 	[%SP+24], %r7;
	st.u32 	[%SP+28], %r8;
	ld.u32 	%r9, [%SP+16];
	st.param.b32 	[func_retval0+0], %r9;
	ret;

}
	// .globl	_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv
.visible .entry _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv(
	.param .u64 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_0,
	.param .u64 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_1,
	.param .u64 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_2,
	.param .u64 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_3,
	.param .u64 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_4,
	.param .u32 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_5,
	.param .u32 _D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_6
)
{
	.local .align 8 .b8 	__local_depot2[232];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<15>;
	.reg .b32 	%r<116>;
	.reg .b64 	%rd<101>;

	mov.u64 	%SPL, __local_depot2;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r2, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_6];
	ld.param.u32 	%r1, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_5];
	ld.param.u64 	%rd5, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_4];
	ld.param.u64 	%rd4, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_3];
	ld.param.u64 	%rd3, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_2];
	ld.param.u64 	%rd2, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_1];
	ld.param.u64 	%rd1, [_D7dispmap21stereoDisparityKernelFmmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmiiZv_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	st.u64 	[%SP+24], %rd4;
	st.u64 	[%SP+32], %rd5;
	st.u32 	[%SP+40], %r1;
	st.u32 	[%SP+44], %r2;
	{ // callseq 0, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11GlobalIndex__T1xZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd6, [retval0+0];
	} // callseq 0
	st.u32 	[%SP+48], %rd6;
	{ // callseq 1, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11GlobalIndex__T1yZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd8, [retval0+0];
	} // callseq 1
	st.u32 	[%SP+52], %rd8;
	{ // callseq 2, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11SharedIndex__T1xZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd10, [retval0+0];
	} // callseq 2
	add.s64 	%rd12, %rd10, 8;
	st.u64 	[%SP+56], %rd12;
	{ // callseq 3, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11SharedIndex__T1yZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd13, [retval0+0];
	} // callseq 3
	add.s64 	%rd15, %rd13, 8;
	st.u64 	[%SP+64], %rd15;
	mov.u32 	%r3, 0;
	st.u32 	[%SP+72], %r3;
	st.u32 	[%SP+76], %r3;
	st.u32 	[%SP+80], %r3;
	mov.u32 	%r4, 9999999;
	st.u32 	[%SP+84], %r4;
	st.u32 	[%SP+88], %r3;
	add.u64 	%rd16, %SP, 96;
	{ // callseq 4, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd16;
	call.uni 
	_D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe, 
	(
	param0
	);
	} // callseq 4
	st.u32 	[%SP+112], %r3;
	mov.u64 	%rd17, 0;
	st.u64 	[%SP+104], %rd17;
	st.u32 	[%SP+128], %r3;
	st.u64 	[%SP+120], %rd17;
	add.u64 	%rd18, %SP, 104;
	st.u64 	[%SP+136], %rd18;
	add.u64 	%rd19, %SP, 120;
	st.u64 	[%SP+144], %rd19;
	st.u32 	[%SP+152], %r3;
	mov.u32 	%r5, 3;
	st.u32 	[%SP+156], %r5;
	bra.uni 	LBB2_1;
LBB2_1:
	ld.u32 	%r6, [%SP+152];
	ld.u32 	%r7, [%SP+156];
	setp.ge.s32 	%p3, %r6, %r7;
	@%p3 bra 	LBB2_4;
	bra.uni 	LBB2_2;
LBB2_2:
	ld.u32 	%r96, [%SP+152];
	st.u32 	[%SP+160], %r96;
	ld.u32 	%r97, [%SP+160];
	shl.b32 	%r98, %r97, 3;
	add.s32 	%r99, %r98, -8;
	st.u32 	[%SP+164], %r99;
	ld.s32 	%rd91, [%SP+160];
	ld.u64 	%rd92, [%SP+136];
	shl.b64 	%rd93, %rd91, 2;
	add.s64 	%rd94, %rd92, %rd93;
	ld.u64 	%rd95, [%SP+0];
	ld.u32 	%r100, [%SP+48];
	add.s32 	%r101, %r100, -8;
	ld.u32 	%r102, [%SP+52];
	ld.u32 	%r103, [%SP+164];
	add.s32 	%r104, %r102, %r103;
	{ // callseq 10, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd95;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r101;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r104;
	.param .b32 retval0;
	call.uni (retval0), 
	_D7dispmap5tex2DFmiiZk, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r105, [retval0+0];
	} // callseq 10
	st.u32 	[%rd94], %r105;
	ld.s32 	%rd96, [%SP+160];
	ld.u64 	%rd97, [%SP+144];
	shl.b64 	%rd98, %rd96, 2;
	add.s64 	%rd99, %rd97, %rd98;
	ld.u64 	%rd100, [%SP+0];
	ld.u32 	%r107, [%SP+48];
	add.s32 	%r108, %r107, 24;
	ld.u32 	%r109, [%SP+52];
	ld.u32 	%r110, [%SP+164];
	add.s32 	%r111, %r109, %r110;
	{ // callseq 11, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd100;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r108;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r111;
	.param .b32 retval0;
	call.uni (retval0), 
	_D7dispmap5tex2DFmiiZk, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r112, [retval0+0];
	} // callseq 11
	st.u32 	[%rd99], %r112;
	bra.uni 	LBB2_3;
LBB2_3:
	ld.u32 	%r114, [%SP+152];
	add.s32 	%r115, %r114, 1;
	st.u32 	[%SP+152], %r115;
	bra.uni 	LBB2_1;
LBB2_4:
	ld.u32 	%r8, [%SP+40];
	st.u32 	[%SP+168], %r8;
	bra.uni 	LBB2_5;
LBB2_5:
	ld.u32 	%r9, [%SP+168];
	ld.u32 	%r10, [%SP+44];
	setp.gt.s32 	%p4, %r9, %r10;
	@%p4 bra 	LBB2_32;
	bra.uni 	LBB2_6;
LBB2_6:
	mov.u32 	%r12, 0;
	st.u32 	[%SP+172], %r12;
	mov.u32 	%r13, 3;
	st.u32 	[%SP+176], %r13;
	bra.uni 	LBB2_7;
LBB2_7:
	ld.u32 	%r14, [%SP+172];
	ld.u32 	%r15, [%SP+176];
	setp.ge.s32 	%p7, %r14, %r15;
	@%p7 bra 	LBB2_10;
	bra.uni 	LBB2_8;
LBB2_8:
	ld.u32 	%r75, [%SP+172];
	st.u32 	[%SP+180], %r75;
	ld.u32 	%r76, [%SP+180];
	shl.b32 	%r77, %r76, 3;
	add.s32 	%r78, %r77, -8;
	st.u32 	[%SP+184], %r78;
	ld.s32 	%rd77, [%SP+180];
	ld.u64 	%rd78, [%SP+136];
	shl.b64 	%rd79, %rd77, 2;
	add.s64 	%rd80, %rd78, %rd79;
	ld.u32 	%r79, [%rd80];
	st.u32 	[%SP+72], %r79;
	ld.u64 	%rd81, [%SP+8];
	ld.u32 	%r80, [%SP+48];
	ld.u32 	%r81, [%SP+168];
	add.s32 	%r82, %r80, %r81;
	add.s32 	%r83, %r82, -8;
	ld.u32 	%r84, [%SP+52];
	ld.u32 	%r85, [%SP+184];
	add.s32 	%r86, %r84, %r85;
	{ // callseq 8, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd81;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r83;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r86;
	.param .b32 retval0;
	call.uni (retval0), 
	_D7dispmap5tex2DFmiiZk, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r87, [retval0+0];
	} // callseq 8
	st.u32 	[%SP+76], %r87;
	ld.u64 	%rd82, [%SP+64];
	ld.s32 	%rd83, [%SP+184];
	add.s64 	%rd84, %rd82, %rd83;
	mul.lo.s64 	%rd85, %rd84, 48;
	ld.u64 	%rd86, [%SP+56];
	add.s64 	%rd87, %rd86, %rd85;
	shl.b64 	%rd88, %rd87, 2;
	ld.u64 	%rd89, [%SP+96];
	add.s64 	%rd90, %rd88, %rd89;
	ld.u32 	%r89, [%SP+72];
	ld.u32 	%r90, [%SP+76];
	mov.u32 	%r91, 0;
	{ // callseq 9, 0
	.reg .b32 temp_param_reg;
	.param .b32 param0;
	st.param.b32 	[param0+0], %r89;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r90;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r91;
	.param .b32 retval0;
	call.uni (retval0), 
	_D7dispmap7__usad4FkkkZk, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r92, [retval0+0];
	} // callseq 9
	st.shared.u32 	[%rd90+-32], %r92;
	bra.uni 	LBB2_9;
LBB2_9:
	ld.u32 	%r94, [%SP+172];
	add.s32 	%r95, %r94, 1;
	st.u32 	[%SP+172], %r95;
	bra.uni 	LBB2_7;
LBB2_10:
	mov.u32 	%r16, 0;
	st.u32 	[%SP+188], %r16;
	mov.u32 	%r17, 3;
	st.u32 	[%SP+192], %r17;
	bra.uni 	LBB2_11;
LBB2_11:
	ld.u32 	%r18, [%SP+188];
	ld.u32 	%r19, [%SP+192];
	setp.ge.s32 	%p8, %r18, %r19;
	@%p8 bra 	LBB2_16;
	bra.uni 	LBB2_12;
LBB2_12:
	ld.u32 	%r54, [%SP+188];
	st.u32 	[%SP+196], %r54;
	ld.u32 	%r55, [%SP+196];
	shl.b32 	%r56, %r55, 3;
	add.s32 	%r57, %r56, -8;
	st.u32 	[%SP+200], %r57;
	{ // callseq 5, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11SharedIndex__T1xZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd61, [retval0+0];
	} // callseq 5
	setp.gt.u64 	%p13, %rd61, 15;
	@%p13 bra 	LBB2_14;
	bra.uni 	LBB2_13;
LBB2_13:
	ld.s32 	%rd63, [%SP+196];
	ld.u64 	%rd64, [%SP+144];
	shl.b64 	%rd65, %rd63, 2;
	add.s64 	%rd66, %rd64, %rd65;
	ld.u32 	%r58, [%rd66];
	st.u32 	[%SP+72], %r58;
	ld.u64 	%rd67, [%SP+8];
	ld.u32 	%r59, [%SP+48];
	ld.u32 	%r60, [%SP+168];
	add.s32 	%r61, %r59, %r60;
	add.s32 	%r62, %r61, 24;
	ld.u32 	%r63, [%SP+52];
	ld.u32 	%r64, [%SP+200];
	add.s32 	%r65, %r63, %r64;
	{ // callseq 6, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd67;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r62;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r65;
	.param .b32 retval0;
	call.uni (retval0), 
	_D7dispmap5tex2DFmiiZk, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r66, [retval0+0];
	} // callseq 6
	st.u32 	[%SP+76], %r66;
	ld.u64 	%rd68, [%SP+64];
	ld.s32 	%rd69, [%SP+200];
	add.s64 	%rd70, %rd68, %rd69;
	mul.lo.s64 	%rd71, %rd70, 48;
	ld.u64 	%rd72, [%SP+56];
	add.s64 	%rd73, %rd72, %rd71;
	shl.b64 	%rd74, %rd73, 2;
	ld.u64 	%rd75, [%SP+96];
	add.s64 	%rd76, %rd74, %rd75;
	ld.u32 	%r68, [%SP+72];
	ld.u32 	%r69, [%SP+76];
	mov.u32 	%r70, 0;
	{ // callseq 7, 0
	.reg .b32 temp_param_reg;
	.param .b32 param0;
	st.param.b32 	[param0+0], %r68;
	.param .b32 param1;
	st.param.b32 	[param1+0], %r69;
	.param .b32 param2;
	st.param.b32 	[param2+0], %r70;
	.param .b32 retval0;
	call.uni (retval0), 
	_D7dispmap7__usad4FkkkZk, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.b32 	%r71, [retval0+0];
	} // callseq 7
	st.shared.u32 	[%rd76+96], %r71;
	bra.uni 	LBB2_14;
LBB2_14:
	bra.uni 	LBB2_15;
LBB2_15:
	ld.u32 	%r73, [%SP+188];
	add.s32 	%r74, %r73, 1;
	st.u32 	[%SP+188], %r74;
	bra.uni 	LBB2_11;
LBB2_16:
	bar.sync 	0;
	mov.u32 	%r20, 0;
	st.u32 	[%SP+204], %r20;
	mov.u32 	%r21, 3;
	st.u32 	[%SP+208], %r21;
	bra.uni 	LBB2_17;
LBB2_17:
	ld.u32 	%r22, [%SP+204];
	ld.u32 	%r23, [%SP+208];
	setp.ge.s32 	%p9, %r22, %r23;
	@%p9 bra 	LBB2_24;
	bra.uni 	LBB2_18;
LBB2_18:
	ld.u32 	%r39, [%SP+204];
	st.u32 	[%SP+212], %r39;
	ld.u32 	%r40, [%SP+212];
	shl.b32 	%r41, %r40, 3;
	add.s32 	%r42, %r41, -8;
	st.u32 	[%SP+216], %r42;
	mov.u32 	%r43, 0;
	st.u32 	[%SP+80], %r43;
	mov.u32 	%r44, -8;
	st.u32 	[%SP+220], %r44;
	bra.uni 	LBB2_19;
LBB2_19:
	ld.u32 	%r45, [%SP+220];
	setp.gt.s32 	%p12, %r45, 8;
	@%p12 bra 	LBB2_22;
	bra.uni 	LBB2_20;
LBB2_20:
	ld.u64 	%rd50, [%SP+64];
	ld.s32 	%rd51, [%SP+216];
	add.s64 	%rd52, %rd50, %rd51;
	mul.lo.s64 	%rd53, %rd52, 48;
	ld.u64 	%rd54, [%SP+56];
	ld.s32 	%rd55, [%SP+220];
	add.s64 	%rd56, %rd54, %rd55;
	add.s64 	%rd57, %rd53, %rd56;
	ld.u64 	%rd58, [%SP+96];
	shl.b64 	%rd59, %rd57, 2;
	add.s64 	%rd60, %rd58, %rd59;
	ld.shared.u32 	%r49, [%rd60];
	ld.u32 	%r50, [%SP+80];
	add.s32 	%r51, %r50, %r49;
	st.u32 	[%SP+80], %r51;
	bra.uni 	LBB2_21;
LBB2_21:
	ld.u32 	%r52, [%SP+220];
	add.s32 	%r53, %r52, 1;
	st.u32 	[%SP+220], %r53;
	bra.uni 	LBB2_19;
LBB2_22:
	bar.sync 	0;
	ld.u64 	%rd41, [%SP+64];
	ld.s32 	%rd42, [%SP+216];
	add.s64 	%rd43, %rd41, %rd42;
	mul.lo.s64 	%rd44, %rd43, 48;
	ld.u64 	%rd45, [%SP+56];
	add.s64 	%rd46, %rd44, %rd45;
	ld.u64 	%rd47, [%SP+96];
	shl.b64 	%rd48, %rd46, 2;
	add.s64 	%rd49, %rd47, %rd48;
	ld.u32 	%r46, [%SP+80];
	st.shared.u32 	[%rd49], %r46;
	bar.sync 	0;
	bra.uni 	LBB2_23;
LBB2_23:
	ld.u32 	%r47, [%SP+204];
	add.s32 	%r48, %r47, 1;
	st.u32 	[%SP+204], %r48;
	bra.uni 	LBB2_17;
LBB2_24:
	mov.u32 	%r24, 0;
	st.u32 	[%SP+80], %r24;
	mov.u32 	%r25, -8;
	st.u32 	[%SP+224], %r25;
	bra.uni 	LBB2_25;
LBB2_25:
	ld.u32 	%r26, [%SP+224];
	setp.gt.s32 	%p10, %r26, 8;
	@%p10 bra 	LBB2_28;
	bra.uni 	LBB2_26;
LBB2_26:
	ld.u64 	%rd32, [%SP+64];
	ld.s32 	%rd33, [%SP+224];
	add.s64 	%rd34, %rd32, %rd33;
	mul.lo.s64 	%rd35, %rd34, 48;
	ld.u64 	%rd36, [%SP+56];
	add.s64 	%rd37, %rd35, %rd36;
	ld.u64 	%rd38, [%SP+96];
	shl.b64 	%rd39, %rd37, 2;
	add.s64 	%rd40, %rd38, %rd39;
	ld.shared.u32 	%r34, [%rd40];
	ld.u32 	%r35, [%SP+80];
	add.s32 	%r36, %r35, %r34;
	st.u32 	[%SP+80], %r36;
	bra.uni 	LBB2_27;
LBB2_27:
	ld.u32 	%r37, [%SP+224];
	add.s32 	%r38, %r37, 1;
	st.u32 	[%SP+224], %r38;
	bra.uni 	LBB2_25;
LBB2_28:
	ld.u32 	%r27, [%SP+80];
	ld.u32 	%r28, [%SP+84];
	setp.ge.u32 	%p11, %r27, %r28;
	@%p11 bra 	LBB2_30;
	bra.uni 	LBB2_29;
LBB2_29:
	ld.u32 	%r29, [%SP+80];
	st.u32 	[%SP+84], %r29;
	ld.u32 	%r30, [%SP+168];
	add.s32 	%r31, %r30, 8;
	st.u32 	[%SP+88], %r31;
	bra.uni 	LBB2_30;
LBB2_30:
	bar.sync 	0;
	bra.uni 	LBB2_31;
LBB2_31:
	ld.u32 	%r32, [%SP+168];
	add.s32 	%r33, %r32, 1;
	st.u32 	[%SP+168], %r33;
	bra.uni 	LBB2_5;
LBB2_32:
	ld.s32 	%rd20, [%SP+52];
	ld.u64 	%rd21, [%SP+32];
	setp.ge.u64 	%p6, %rd20, %rd21;
	mov.pred 	%p5, 0;
	mov.pred 	%p14, %p5;
	@%p6 bra 	LBB2_34;
	bra.uni 	LBB2_33;
LBB2_33:
	ld.s32 	%rd22, [%SP+48];
	ld.u64 	%rd23, [%SP+24];
	setp.lt.u64 	%p1, %rd22, %rd23;
	mov.pred 	%p14, %p1;
	bra.uni 	LBB2_34;
LBB2_34:
	mov.pred 	%p2, %p14;
	@!%p2 bra 	LBB2_36;
	bra.uni 	LBB2_35;
LBB2_35:
	ld.s32 	%rd24, [%SP+52];
	ld.u64 	%rd25, [%SP+24];
	mul.lo.s64 	%rd26, %rd24, %rd25;
	ld.s32 	%rd27, [%SP+48];
	add.s64 	%rd28, %rd26, %rd27;
	ld.u64 	%rd29, [%SP+16];
	shl.b64 	%rd30, %rd28, 2;
	add.s64 	%rd31, %rd29, %rd30;
	ld.u32 	%r11, [%SP+88];
	st.global.u32 	[%rd31], %r11;
	bra.uni 	LBB2_36;
LBB2_36:
	ret;

}
	// .weak	_D8dcompute3std5index11GlobalIndex__T1xZQdFNaNbNdNiZm
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11GlobalIndex__T1xZQdFNaNbNdNiZm()
{
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<2>;

	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mul.lo.s32 	%r3, %r1, %r2;
	mov.u32 	%r4, %tid.x;
	add.s32 	%r5, %r3, %r4;
	cvt.u64.u32 	%rd1, %r5;
	st.param.b64 	[func_retval0+0], %rd1;
	ret;

}
	// .weak	_D8dcompute3std5index11GlobalIndex__T1yZQdFNaNbNdNiZm
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11GlobalIndex__T1yZQdFNaNbNdNiZm()
{
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<2>;

	mov.u32 	%r1, %ctaid.y;
	mov.u32 	%r2, %ntid.y;
	mul.lo.s32 	%r3, %r1, %r2;
	mov.u32 	%r4, %tid.y;
	add.s32 	%r5, %r3, %r4;
	cvt.u64.u32 	%rd1, %r5;
	st.param.b64 	[func_retval0+0], %rd1;
	ret;

}
	// .weak	_D8dcompute3std5index11SharedIndex__T1xZQdFNaNbNdNiZm
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11SharedIndex__T1xZQdFNaNbNdNiZm()
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<2>;

	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	st.param.b64 	[func_retval0+0], %rd1;
	ret;

}
	// .weak	_D8dcompute3std5index11SharedIndex__T1yZQdFNaNbNdNiZm
.weak .func  (.param .b64 func_retval0) _D8dcompute3std5index11SharedIndex__T1yZQdFNaNbNdNiZm()
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<2>;

	mov.u32 	%r1, %tid.y;
	cvt.u64.u32 	%rd1, %r1;
	st.param.b64 	[func_retval0+0], %rd1;
	ret;

}
	// .weak	_D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe
.weak .func _D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe(
	.param .b64 _D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe_param_0
)
{
	.local .align 8 .b8 	__local_depot7[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<9>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot7;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u64 	%rd1, [_D8dcompute3std6memory__T19sharedStaticReserveHTG1152kVAyaa5_6469666630Vmi1152ZQCcFNbNiZS3ldcQDn__T7PointerVEQuQEf9AddrSpacei2TkZQBe_param_0];
	mov.u64 	%rd2, diff0;
	st.u64 	[%SP+0], %rd2;
	add.u64 	%rd3, %SP, 0;
	st.u64 	[%SP+8], %rd3;
	ld.u64 	%rd4, [%SP+8];
	ld.u8 	%rs1, [%rd4+7];
	st.u8 	[%rd1+7], %rs1;
	ld.u8 	%rs2, [%rd4+6];
	st.u8 	[%rd1+6], %rs2;
	ld.u8 	%rs3, [%rd4+5];
	st.u8 	[%rd1+5], %rs3;
	ld.u8 	%rs4, [%rd4+4];
	st.u8 	[%rd1+4], %rs4;
	ld.u8 	%rs5, [%rd4+3];
	st.u8 	[%rd1+3], %rs5;
	ld.u8 	%rs6, [%rd4+2];
	st.u8 	[%rd1+2], %rs6;
	ld.u8 	%rs7, [%rd4+1];
	st.u8 	[%rd1+1], %rs7;
	ld.u8 	%rs8, [%rd4];
	st.u8 	[%rd1], %rs8;
	ret;

}
	// .globl	_D7dispmap8printIntFkZv
.visible .func _D7dispmap8printIntFkZv(
	.param .b32 _D7dispmap8printIntFkZv_param_0
)
{
	.local .align 8 .b8 	__local_depot8[56];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<5>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot8;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [_D7dispmap8printIntFkZv_param_0];
	st.u32 	[%SP+48], %r1;
	ld.u32 	%r2, [%SP+48];
	add.u64 	%rd1, %SP, 0;
	cvta.to.local.u64 	%rd2, %rd1;
	st.local.u32 	[%rd2], %r2;
	mov.u64 	%rd3, str;
	cvta.const.u64 	%rd4, %rd3;
	{ // callseq 12, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd4;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd1;
	.param .b32 retval0;
	call.uni (retval0), 
	vprintf, 
	(
	param0, 
	param1
	);
	ld.param.b32 	%r3, [retval0+0];
	} // callseq 12
	ret;

}
	// .weak	_D8dcompute3std6memory__T18constStaticReserveHTG64fVAyaa6_676175737330Vmi64ZQBzFNbNiNfZPyf
.weak .func  (.param .b64 func_retval0) _D8dcompute3std6memory__T18constStaticReserveHTG64fVAyaa6_676175737330Vmi64ZQBzFNbNiNfZPyf()
{
	.local .align 8 .b8 	__local_depot9[8];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot9;
	cvta.local.u64 	%SP, %SPL;
	mov.u64 	%rd1, gauss0;
	cvta.const.u64 	%rd2, %rd1;
	st.u64 	[%SP+0], %rd2;
	ld.u64 	%rd3, [%SP+0];
	st.param.b64 	[func_retval0+0], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T4ItoaVki64ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T4ItoaVki64ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 2;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T4ItoaVki6ZQkFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T4ItoaVki6ZQkFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str1;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T5DigitVmi6ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T5DigitVmi6ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str1;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T5DigitVmi4ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T5DigitVmi4ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str2;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T8llvmTypeTfZQmFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T8llvmTypeTfZQmFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 5;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str3;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe
.weak .func _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe(
	.param .b64 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_0,
	.param .b64 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_1,
	.param .b32 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_2
)
{
	.local .align 4 .b8 	__local_depot15[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<14>;
	.reg .b64 	%rd<3>;

	mov.u64 	%SPL, __local_depot15;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f1, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_2];
	ld.param.u64 	%rd2, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_1];
	ld.param.u64 	%rd1, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_0];
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f2, [%rd2];
	ld.f32 	%f3, [%SP+0];
	mul.rn.f32 	%f4, %f2, %f3;
	st.f32 	[%rd1], %f4;
	ld.f32 	%f5, [%rd2+4];
	ld.f32 	%f6, [%SP+0];
	mul.rn.f32 	%f7, %f5, %f6;
	st.f32 	[%rd1+4], %f7;
	ld.f32 	%f8, [%rd2+8];
	ld.f32 	%f9, [%SP+0];
	mul.rn.f32 	%f10, %f8, %f9;
	st.f32 	[%rd1+8], %f10;
	ld.f32 	%f11, [%rd2+12];
	ld.f32 	%f12, [%SP+0];
	mul.rn.f32 	%f13, %f11, %f12;
	st.f32 	[%rd1+12], %f13;
	ret;

}
	// .weak	_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr
.weak .func _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr(
	.param .b64 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_0,
	.param .b64 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_1,
	.param .align 4 .b8 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_2[16]
)
{
	.local .align 8 .b8 	__local_depot16[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<17>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot16;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f4, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_2+12];
	ld.param.f32 	%f3, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_2+8];
	ld.param.f32 	%f2, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_2+4];
	ld.param.f32 	%f1, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_2];
	ld.param.u64 	%rd2, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_1];
	ld.param.u64 	%rd1, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr_param_0];
	add.u64 	%rd3, %SP, 0;
	or.b64  	%rd4, %rd3, 4;
	st.f32 	[%rd4], %f2;
	st.f32 	[%SP+12], %f4;
	st.f32 	[%SP+8], %f3;
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f5, [%rd2];
	ld.f32 	%f6, [%SP+0];
	add.rn.f32 	%f7, %f5, %f6;
	st.f32 	[%rd1], %f7;
	ld.f32 	%f8, [%rd2+4];
	ld.f32 	%f9, [%rd4];
	add.rn.f32 	%f10, %f8, %f9;
	st.f32 	[%rd1+4], %f10;
	ld.f32 	%f11, [%rd2+8];
	ld.f32 	%f12, [%SP+8];
	add.rn.f32 	%f13, %f11, %f12;
	st.f32 	[%rd1+8], %f13;
	ld.f32 	%f14, [%rd2+12];
	ld.f32 	%f15, [%SP+12];
	add.rn.f32 	%f16, %f14, %f15;
	st.f32 	[%rd1+12], %f16;
	ret;

}
	// .weak	_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe
.weak .func _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe(
	.param .b64 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_0,
	.param .b64 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_1,
	.param .b32 _D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_2
)
{
	.local .align 4 .b8 	__local_depot17[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<14>;
	.reg .b64 	%rd<3>;

	mov.u64 	%SPL, __local_depot17;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f1, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_2];
	ld.param.u64 	%rd2, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_1];
	ld.param.u64 	%rd1, [_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe_param_0];
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f2, [%rd2];
	ld.f32 	%f3, [%SP+0];
	div.rn.f32 	%f4, %f2, %f3;
	st.f32 	[%rd1], %f4;
	ld.f32 	%f5, [%rd2+4];
	ld.f32 	%f6, [%SP+0];
	div.rn.f32 	%f7, %f5, %f6;
	st.f32 	[%rd1+4], %f7;
	ld.f32 	%f8, [%rd2+8];
	ld.f32 	%f9, [%SP+0];
	div.rn.f32 	%f10, %f8, %f9;
	st.f32 	[%rd1+8], %f10;
	ld.f32 	%f11, [%rd2+12];
	ld.f32 	%f12, [%SP+0];
	div.rn.f32 	%f13, %f11, %f12;
	st.f32 	[%rd1+12], %f13;
	ret;

}
	// .weak	_D8dcompute3std6memory__T4ItoaVki1152ZQnFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T4ItoaVki1152ZQnFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 4;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str4;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T4ItoaVki115ZQmFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T4ItoaVki115ZQmFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 3;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str5;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T4ItoaVki11ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T4ItoaVki11ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 2;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str6;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T4ItoaVki1ZQkFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T4ItoaVki1ZQkFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str7;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T5DigitVmi1ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T5DigitVmi1ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str7;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T5DigitVmi5ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T5DigitVmi5ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str8;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T5DigitVmi2ZQlFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T5DigitVmi2ZQlFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 1;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str9;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .weak	_D8dcompute3std6memory__T8llvmTypeTkZQmFNaNbNiNfZyAa
.weak .func  (.param .align 8 .b8 func_retval0[16]) _D8dcompute3std6memory__T8llvmTypeTkZQmFNaNbNiNfZyAa()
{
	.reg .b64 	%rd<4>;

	mov.u64 	%rd1, 3;
	st.param.b64 	[func_retval0+0], %rd1;
	mov.u64 	%rd2, _$_str10;
	cvta.global.u64 	%rd3, %rd2;
	st.param.b64 	[func_retval0+8], %rd3;
	ret;

}
	// .globl	_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf
.visible .func  (.param .b32 func_retval0) _D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf(
	.param .align 4 .b8 _D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_0[16],
	.param .align 4 .b8 _D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_1[16],
	.param .b32 _D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_2
)
{
	.local .align 8 .b8 	__local_depot26[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .f32 	%f<31>;
	.reg .b64 	%rd<5>;

	mov.u64 	%SPL, __local_depot26;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f8, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_1+12];
	ld.param.f32 	%f7, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_1+8];
	ld.param.f32 	%f6, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_1+4];
	ld.param.f32 	%f5, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_1];
	ld.param.f32 	%f4, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_0+12];
	ld.param.f32 	%f3, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_0+8];
	ld.param.f32 	%f2, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_0+4];
	ld.param.f32 	%f1, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_0];
	ld.param.f32 	%f9, [_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf_param_2];
	add.u64 	%rd1, %SP, 0;
	or.b64  	%rd2, %rd1, 4;
	st.f32 	[%rd2], %f2;
	st.f32 	[%SP+12], %f4;
	st.f32 	[%SP+8], %f3;
	st.f32 	[%SP+0], %f1;
	add.u64 	%rd3, %SP, 16;
	or.b64  	%rd4, %rd3, 4;
	st.f32 	[%rd4], %f6;
	st.f32 	[%SP+28], %f8;
	st.f32 	[%SP+24], %f7;
	st.f32 	[%SP+16], %f5;
	st.f32 	[%SP+32], %f9;
	ld.f32 	%f10, [%SP+16];
	ld.f32 	%f11, [%SP+0];
	sub.rn.f32 	%f12, %f10, %f11;
	mul.rn.f32 	%f13, %f12, %f12;
	ld.f32 	%f14, [%rd4];
	ld.f32 	%f15, [%rd2];
	sub.rn.f32 	%f16, %f14, %f15;
	mul.rn.f32 	%f17, %f16, %f16;
	add.rn.f32 	%f18, %f13, %f17;
	ld.f32 	%f19, [%SP+24];
	ld.f32 	%f20, [%SP+8];
	sub.rn.f32 	%f21, %f19, %f20;
	mul.rn.f32 	%f22, %f21, %f21;
	add.rn.f32 	%f23, %f18, %f22;
	st.f32 	[%SP+36], %f23;
	ld.f32 	%f24, [%SP+36];
	neg.f32 	%f25, %f24;
	ld.f32 	%f26, [%SP+32];
	add.rn.f32 	%f27, %f26, %f26;
	mul.rn.f32 	%f28, %f27, %f26;
	div.rn.f32 	%f29, %f25, %f28;
	ex2.approx.f32 	%f30, %f29;
	st.param.f32 	[func_retval0+0], %f30;
	ret;

}
	// .globl	_D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk
.visible .func  (.param .b32 func_retval0) _D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk(
	.param .align 4 .b8 _D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk_param_0[16]
)
{
	.local .align 8 .b8 	__local_depot27[16];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b32 	%r<11>;
	.reg .f32 	%f<29>;
	.reg .b64 	%rd<3>;

	mov.u64 	%SPL, __local_depot27;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f4, [_D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk_param_0+12];
	ld.param.f32 	%f3, [_D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk_param_0+8];
	ld.param.f32 	%f2, [_D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk_param_0+4];
	ld.param.f32 	%f1, [_D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk_param_0];
	add.u64 	%rd1, %SP, 0;
	or.b64  	%rd2, %rd1, 4;
	st.f32 	[%rd2], %f2;
	st.f32 	[%SP+12], %f4;
	st.f32 	[%SP+8], %f3;
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f5, [%SP+0];
	{ // callseq 13, 0
	.reg .b32 temp_param_reg;
	.param .b32 param0;
	st.param.f32 	[param0+0], %f5;
	.param .b32 retval0;
	call.uni (retval0), 
	_D9bilateral__T3absTfZQhFNaNbNiNefZf, 
	(
	param0
	);
	ld.param.f32 	%f6, [retval0+0];
	} // callseq 13
	cvt.sat.f32.f32 	%f8, %f6;
	st.f32 	[%SP+0], %f8;
	ld.f32 	%f9, [%rd2];
	{ // callseq 14, 0
	.reg .b32 temp_param_reg;
	.param .b32 param0;
	st.param.f32 	[param0+0], %f9;
	.param .b32 retval0;
	call.uni (retval0), 
	_D9bilateral__T3absTfZQhFNaNbNiNefZf, 
	(
	param0
	);
	ld.param.f32 	%f10, [retval0+0];
	} // callseq 14
	cvt.sat.f32.f32 	%f12, %f10;
	st.f32 	[%rd2], %f12;
	ld.f32 	%f13, [%SP+8];
	{ // callseq 15, 0
	.reg .b32 temp_param_reg;
	.param .b32 param0;
	st.param.f32 	[param0+0], %f13;
	.param .b32 retval0;
	call.uni (retval0), 
	_D9bilateral__T3absTfZQhFNaNbNiNefZf, 
	(
	param0
	);
	ld.param.f32 	%f14, [retval0+0];
	} // callseq 15
	cvt.sat.f32.f32 	%f16, %f14;
	st.f32 	[%SP+8], %f16;
	ld.f32 	%f17, [%SP+12];
	{ // callseq 16, 0
	.reg .b32 temp_param_reg;
	.param .b32 param0;
	st.param.f32 	[param0+0], %f17;
	.param .b32 retval0;
	call.uni (retval0), 
	_D9bilateral__T3absTfZQhFNaNbNiNefZf, 
	(
	param0
	);
	ld.param.f32 	%f18, [retval0+0];
	} // callseq 16
	cvt.sat.f32.f32 	%f20, %f18;
	st.f32 	[%SP+12], %f20;
	ld.f32 	%f21, [%SP+12];
	mul.rn.f32 	%f22, %f21, 0f437F0000;
	cvt.rzi.u32.f32 	%r1, %f22;
	shl.b32 	%r2, %r1, 24;
	ld.f32 	%f23, [%SP+8];
	mul.rn.f32 	%f24, %f23, 0f437F0000;
	cvt.rzi.u32.f32 	%r3, %f24;
	shl.b32 	%r4, %r3, 16;
	or.b32  	%r5, %r2, %r4;
	ld.f32 	%f25, [%rd2];
	mul.rn.f32 	%f26, %f25, 0f437F0000;
	cvt.rzi.u32.f32 	%r6, %f26;
	shl.b32 	%r7, %r6, 8;
	or.b32  	%r8, %r5, %r7;
	ld.f32 	%f27, [%SP+0];
	mul.rn.f32 	%f28, %f27, 0f437F0000;
	cvt.rzi.u32.f32 	%r9, %f28;
	or.b32  	%r10, %r8, %r9;
	st.param.b32 	[func_retval0+0], %r10;
	ret;

}
	// .weak	_D9bilateral__T3absTfZQhFNaNbNiNefZf
.weak .func  (.param .b32 func_retval0) _D9bilateral__T3absTfZQhFNaNbNiNefZf(
	.param .b32 _D9bilateral__T3absTfZQhFNaNbNiNefZf_param_0
)
{
	.local .align 8 .b8 	__local_depot28[24];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<2>;
	.reg .f32 	%f<6>;
	.reg .b64 	%rd<4>;

	mov.u64 	%SPL, __local_depot28;
	cvta.local.u64 	%SP, %SPL;
	ld.param.f32 	%f1, [_D9bilateral__T3absTfZQhFNaNbNiNefZf_param_0];
	st.f32 	[%SP+0], %f1;
	ld.f32 	%f2, [%SP+0];
	setp.ltu.f32 	%p1, %f2, 0f00000000;
	@%p1 bra 	LBB28_2;
	bra.uni 	LBB28_1;
LBB28_1:
	add.u64 	%rd2, %SP, 0;
	st.u64 	[%SP+8], %rd2;
	bra.uni 	LBB28_3;
LBB28_2:
	ld.f32 	%f3, [%SP+0];
	neg.f32 	%f4, %f3;
	st.f32 	[%SP+16], %f4;
	add.u64 	%rd1, %SP, 16;
	st.u64 	[%SP+8], %rd1;
	bra.uni 	LBB28_3;
LBB28_3:
	ld.u64 	%rd3, [%SP+8];
	ld.f32 	%f5, [%rd3];
	st.param.f32 	[func_retval0+0], %f5;
	ret;

}
	// .globl	_D9bilateral14rgbaIntToFloatFkZS8dcompute3std4cuda7texture6float4
.visible .func _D9bilateral14rgbaIntToFloatFkZS8dcompute3std4cuda7texture6float4(
	.param .b64 _D9bilateral14rgbaIntToFloatFkZS8dcompute3std4cuda7texture6float4_param_0,
	.param .b32 _D9bilateral14rgbaIntToFloatFkZS8dcompute3std4cuda7texture6float4_param_1
)
{
	.local .align 4 .b8 	__local_depot29[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .b16 	%rs<17>;
	.reg .b32 	%r<6>;
	.reg .f32 	%f<9>;
	.reg .b64 	%rd<8>;

	mov.u64 	%SPL, __local_depot29;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [_D9bilateral14rgbaIntToFloatFkZS8dcompute3std4cuda7texture6float4_param_1];
	ld.param.u64 	%rd1, [_D9bilateral14rgbaIntToFloatFkZS8dcompute3std4cuda7texture6float4_param_0];
	st.u32 	[%SP+0], %r1;
	mov.u64 	%rd2, _D8dcompute3std4cuda7texture6float46__initZ;
	cvta.global.u64 	%rd3, %rd2;
	ld.u8 	%rs1, [%rd3+15];
	st.u8 	[%rd1+15], %rs1;
	ld.u8 	%rs2, [%rd3+14];
	st.u8 	[%rd1+14], %rs2;
	ld.u8 	%rs3, [%rd3+13];
	st.u8 	[%rd1+13], %rs3;
	ld.u8 	%rs4, [%rd3+12];
	st.u8 	[%rd1+12], %rs4;
	ld.u8 	%rs5, [%rd3+11];
	st.u8 	[%rd1+11], %rs5;
	ld.u8 	%rs6, [%rd3+10];
	st.u8 	[%rd1+10], %rs6;
	ld.u8 	%rs7, [%rd3+9];
	st.u8 	[%rd1+9], %rs7;
	ld.u8 	%rs8, [%rd3+8];
	st.u8 	[%rd1+8], %rs8;
	ld.u8 	%rs9, [%rd3+7];
	st.u8 	[%rd1+7], %rs9;
	ld.u8 	%rs10, [%rd3+6];
	st.u8 	[%rd1+6], %rs10;
	ld.u8 	%rs11, [%rd3+5];
	st.u8 	[%rd1+5], %rs11;
	ld.u8 	%rs12, [%rd3+4];
	st.u8 	[%rd1+4], %rs12;
	ld.u8 	%rs13, [%rd3+3];
	st.u8 	[%rd1+3], %rs13;
	ld.u8 	%rs14, [%rd3+2];
	st.u8 	[%rd1+2], %rs14;
	ld.u8 	%rs15, [%rd3+1];
	st.u8 	[%rd1+1], %rs15;
	ld.u8 	%rs16, [%rd3];
	st.u8 	[%rd1], %rs16;
	ld.u8 	%r2, [%SP+0];
	cvt.rn.f32.u32 	%f1, %r2;
	mul.rn.f32 	%f2, %f1, 0f3B808081;
	st.f32 	[%rd1], %f2;
	add.u64 	%rd4, %SP, 0;
	or.b64  	%rd5, %rd4, 1;
	ld.u8 	%r3, [%rd5];
	cvt.rn.f32.u32 	%f3, %r3;
	mul.rn.f32 	%f4, %f3, 0f3B808081;
	st.f32 	[%rd1+4], %f4;
	or.b64  	%rd6, %rd4, 2;
	ld.u8 	%r4, [%rd6];
	cvt.rn.f32.u32 	%f5, %r4;
	mul.rn.f32 	%f6, %f5, 0f3B808081;
	st.f32 	[%rd1+8], %f6;
	or.b64  	%rd7, %rd4, 3;
	ld.u8 	%r5, [%rd7];
	cvt.rn.f32.u32 	%f7, %r5;
	mul.rn.f32 	%f8, %f7, 0f3B808081;
	st.f32 	[%rd1+12], %f8;
	ret;

}
	// .globl	_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv
.visible .entry _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv(
	.param .u64 _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_0,
	.param .u64 _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_1,
	.param .u64 _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_2,
	.param .u64 _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_3,
	.param .f32 _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_4,
	.param .u32 _D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_5
)
{
	.local .align 8 .b8 	__local_depot30[200];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<8>;
	.reg .b32 	%r<31>;
	.reg .f32 	%f<38>;
	.reg .b64 	%rd<57>;

	mov.u64 	%SPL, __local_depot30;
	cvta.local.u64 	%SP, %SPL;
	ld.param.u32 	%r1, [_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_5];
	ld.param.f32 	%f1, [_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_4];
	ld.param.u64 	%rd4, [_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_3];
	ld.param.u64 	%rd3, [_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_2];
	ld.param.u64 	%rd2, [_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_1];
	ld.param.u64 	%rd1, [_D9bilateral16bilateral_filterFmS3ldc8dcompute__T7PointerVEQBaQz9AddrSpacei1TiZQBemmfiZv_param_0];
	st.u64 	[%SP+0], %rd1;
	st.u64 	[%SP+8], %rd2;
	st.u64 	[%SP+16], %rd3;
	st.u64 	[%SP+24], %rd4;
	st.f32 	[%SP+32], %f1;
	st.u32 	[%SP+36], %r1;
	{ // callseq 17, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11GlobalIndex__T1xZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd5, [retval0+0];
	} // callseq 17
	st.u32 	[%SP+40], %rd5;
	{ // callseq 18, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std5index11GlobalIndex__T1yZQdFNaNbNdNiZm, 
	(
	);
	ld.param.b64 	%rd7, [retval0+0];
	} // callseq 18
	st.u32 	[%SP+44], %rd7;
	ld.s32 	%rd9, [%SP+40];
	ld.u64 	%rd10, [%SP+16];
	setp.ge.u64 	%p4, %rd9, %rd10;
	mov.pred 	%p3, -1;
	mov.pred 	%p7, %p3;
	@%p4 bra 	LBB30_2;
	bra.uni 	LBB30_1;
LBB30_1:
	ld.s32 	%rd11, [%SP+44];
	ld.u64 	%rd12, [%SP+24];
	setp.ge.u64 	%p1, %rd11, %rd12;
	mov.pred 	%p7, %p1;
	bra.uni 	LBB30_2;
LBB30_2:
	mov.pred 	%p2, %p7;
	@!%p2 bra 	LBB30_4;
	bra.uni 	LBB30_3;
LBB30_3:
	ret;
LBB30_4:
	{ // callseq 19, 0
	.reg .b32 temp_param_reg;
	.param .b64 retval0;
	call.uni (retval0), 
	_D8dcompute3std6memory__T18constStaticReserveHTG64fVAyaa6_676175737330Vmi64ZQBzFNbNiNfZPyf, 
	(
	);
	ld.param.b64 	%rd13, [retval0+0];
	} // callseq 19
	st.u64 	[%SP+48], %rd13;
	mov.u32 	%r2, 0;
	st.u32 	[%SP+56], %r2;
	mov.u32 	%r3, 2143289344;
	st.u32 	[%SP+60], %r3;
	st.u32 	[%SP+64], %r2;
	add.u64 	%rd15, %SP, 64;
	or.b64  	%rd16, %rd15, 4;
	st.u32 	[%rd16], %r2;
	st.u32 	[%SP+72], %r2;
	st.u32 	[%SP+76], %r2;
	ld.u64 	%rd17, [%SP+0];
	ld.u32 	%r4, [%SP+40];
	ld.u32 	%r5, [%SP+44];
	tex.2d.v4.f32.s32 	{%f2, %f3, %f4, %f5}, [%rd17, {%r4, %r5}];
	mov.u64 	%rd18, 0;
	st.u64 	[%SP+104], %rd18;
	st.u64 	[%SP+96], %rd18;
	st.f32 	[%SP+96], %f2;
	add.u64 	%rd19, %SP, 96;
	or.b64  	%rd20, %rd19, 4;
	st.f32 	[%rd20], %f3;
	st.f32 	[%SP+104], %f4;
	st.f32 	[%SP+108], %f5;
	ld.u64 	%rd21, [%SP+104];
	st.u64 	[%SP+88], %rd21;
	ld.u64 	%rd22, [%SP+96];
	st.u64 	[%SP+80], %rd22;
	ld.u32 	%r6, [%SP+36];
	neg.s32 	%r7, %r6;
	st.u32 	[%SP+112], %r7;
	bra.uni 	LBB30_5;
LBB30_5:
	ld.u32 	%r8, [%SP+112];
	ld.u32 	%r9, [%SP+36];
	setp.gt.s32 	%p5, %r8, %r9;
	@%p5 bra 	LBB30_12;
	bra.uni 	LBB30_6;
LBB30_6:
	ld.u32 	%r12, [%SP+36];
	neg.s32 	%r13, %r12;
	st.u32 	[%SP+116], %r13;
	bra.uni 	LBB30_7;
LBB30_7:
	ld.u32 	%r14, [%SP+116];
	ld.u32 	%r15, [%SP+36];
	setp.gt.s32 	%p6, %r14, %r15;
	@%p6 bra 	LBB30_10;
	bra.uni 	LBB30_8;
LBB30_8:
	ld.u64 	%rd34, [%SP+0];
	ld.u32 	%r18, [%SP+40];
	ld.u32 	%r19, [%SP+116];
	add.s32 	%r20, %r18, %r19;
	ld.u32 	%r21, [%SP+44];
	ld.u32 	%r22, [%SP+112];
	add.s32 	%r23, %r21, %r22;
	tex.2d.v4.f32.s32 	{%f11, %f12, %f13, %f14}, [%rd34, {%r20, %r23}];
	mov.u64 	%rd35, 0;
	st.u64 	[%SP+144], %rd35;
	st.u64 	[%SP+136], %rd35;
	st.f32 	[%SP+136], %f11;
	add.u64 	%rd36, %SP, 136;
	or.b64  	%rd37, %rd36, 4;
	st.f32 	[%rd37], %f12;
	st.f32 	[%SP+144], %f13;
	st.f32 	[%SP+148], %f14;
	ld.u64 	%rd38, [%SP+144];
	st.u64 	[%SP+128], %rd38;
	ld.u64 	%rd39, [%SP+136];
	st.u64 	[%SP+120], %rd39;
	ld.u32 	%r24, [%SP+112];
	ld.u32 	%r25, [%SP+36];
	add.s32 	%r26, %r24, %r25;
	cvt.s64.s32 	%rd40, %r26;
	ld.u64 	%rd41, [%SP+48];
	shl.b64 	%rd42, %rd40, 2;
	add.s64 	%rd43, %rd41, %rd42;
	ld.f32 	%f15, [%rd43];
	ld.u32 	%r27, [%SP+116];
	add.s32 	%r28, %r27, %r25;
	cvt.s64.s32 	%rd44, %r28;
	shl.b64 	%rd45, %rd44, 2;
	add.s64 	%rd46, %rd41, %rd45;
	ld.f32 	%f16, [%rd46];
	mul.rn.f32 	%f17, %f15, %f16;
	add.u64 	%rd47, %SP, 120;
	or.b64  	%rd48, %rd47, 4;
	ld.f32 	%f18, [%rd48];
	ld.f32 	%f19, [%SP+132];
	ld.f32 	%f20, [%SP+128];
	ld.f32 	%f21, [%SP+120];
	add.u64 	%rd49, %SP, 80;
	or.b64  	%rd50, %rd49, 4;
	ld.f32 	%f22, [%rd50];
	ld.f32 	%f23, [%SP+92];
	ld.f32 	%f24, [%SP+88];
	ld.f32 	%f25, [%SP+80];
	ld.f32 	%f26, [%SP+32];
	{ // callseq 22, 0
	.reg .b32 temp_param_reg;
	.param .align 4 .b8 param0[16];
	st.param.f32 	[param0+0], %f21;
	st.param.f32 	[param0+4], %f18;
	st.param.f32 	[param0+8], %f20;
	st.param.f32 	[param0+12], %f19;
	.param .align 4 .b8 param1[16];
	st.param.f32 	[param1+0], %f25;
	st.param.f32 	[param1+4], %f22;
	st.param.f32 	[param1+8], %f24;
	st.param.f32 	[param1+12], %f23;
	.param .b32 param2;
	st.param.f32 	[param2+0], %f26;
	.param .b32 retval0;
	call.uni (retval0), 
	_D9bilateral12euclideanLenFS8dcompute3std4cuda7texture6float4QBifZf, 
	(
	param0, 
	param1, 
	param2
	);
	ld.param.f32 	%f27, [retval0+0];
	} // callseq 22
	mul.rn.f32 	%f29, %f17, %f27;
	st.f32 	[%SP+60], %f29;
	ld.f32 	%f30, [%SP+60];
	add.u64 	%rd51, %SP, 168;
	{ // callseq 23, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd51;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd47;
	.param .b32 param2;
	st.param.f32 	[param2+0], %f30;
	call.uni 
	_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2aZQtMFNaNbNiNffZSQCsQCmQClQCjQCe, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 23
	or.b64  	%rd52, %rd51, 4;
	ld.f32 	%f31, [%rd52];
	ld.f32 	%f32, [%SP+180];
	ld.f32 	%f33, [%SP+176];
	ld.f32 	%f34, [%SP+168];
	add.u64 	%rd53, %SP, 152;
	add.u64 	%rd54, %SP, 64;
	{ // callseq 24, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd53;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd54;
	.param .align 4 .b8 param2[16];
	st.param.f32 	[param2+0], %f34;
	st.param.f32 	[param2+4], %f31;
	st.param.f32 	[param2+8], %f33;
	st.param.f32 	[param2+12], %f32;
	call.uni 
	_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2bZQtMFNaNbNiNfSQCqQCkQCjQChQCcZQr, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 24
	ld.u64 	%rd55, [%SP+160];
	st.u64 	[%SP+72], %rd55;
	ld.u64 	%rd56, [%SP+152];
	st.u64 	[%SP+64], %rd56;
	ld.f32 	%f35, [%SP+60];
	ld.f32 	%f36, [%SP+56];
	add.rn.f32 	%f37, %f36, %f35;
	st.f32 	[%SP+56], %f37;
	bra.uni 	LBB30_9;
LBB30_9:
	ld.u32 	%r29, [%SP+116];
	add.s32 	%r30, %r29, 1;
	st.u32 	[%SP+116], %r30;
	bra.uni 	LBB30_7;
LBB30_10:
	bra.uni 	LBB30_11;
LBB30_11:
	ld.u32 	%r16, [%SP+112];
	add.s32 	%r17, %r16, 1;
	st.u32 	[%SP+112], %r17;
	bra.uni 	LBB30_5;
LBB30_12:
	ld.s32 	%rd23, [%SP+44];
	ld.u64 	%rd24, [%SP+16];
	mul.lo.s64 	%rd25, %rd23, %rd24;
	ld.s32 	%rd26, [%SP+40];
	add.s64 	%rd27, %rd25, %rd26;
	ld.u64 	%rd28, [%SP+8];
	shl.b64 	%rd29, %rd27, 2;
	add.s64 	%rd30, %rd28, %rd29;
	ld.f32 	%f6, [%SP+56];
	add.u64 	%rd31, %SP, 184;
	add.u64 	%rd32, %SP, 64;
	{ // callseq 20, 0
	.reg .b32 temp_param_reg;
	.param .b64 param0;
	st.param.b64 	[param0+0], %rd31;
	.param .b64 param1;
	st.param.b64 	[param1+0], %rd32;
	.param .b32 param2;
	st.param.f32 	[param2+0], %f6;
	call.uni 
	_D8dcompute3std4cuda7texture6float4__T8opBinaryVAyaa1_2fZQtMFNaNbNiNffZSQCsQCmQClQCjQCe, 
	(
	param0, 
	param1, 
	param2
	);
	} // callseq 20
	or.b64  	%rd33, %rd31, 4;
	ld.f32 	%f7, [%rd33];
	ld.f32 	%f8, [%SP+196];
	ld.f32 	%f9, [%SP+192];
	ld.f32 	%f10, [%SP+184];
	{ // callseq 21, 0
	.reg .b32 temp_param_reg;
	.param .align 4 .b8 param0[16];
	st.param.f32 	[param0+0], %f10;
	st.param.f32 	[param0+4], %f7;
	st.param.f32 	[param0+8], %f9;
	st.param.f32 	[param0+12], %f8;
	.param .b32 retval0;
	call.uni (retval0), 
	_D9bilateral14rgbaFloatToIntFS8dcompute3std4cuda7texture6float4Zk, 
	(
	param0
	);
	ld.param.b32 	%r10, [retval0+0];
	} // callseq 21
	st.global.u32 	[%rd30], %r10;
	ret;

}
